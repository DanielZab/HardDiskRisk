{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv('Trainingdata.csv', sep=',', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename(columns={dataset.columns[-1]:'testVal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "0.1        0\n",
       "0.2        0\n",
       "0.3        0\n",
       "0.4        0\n",
       "0.6        0\n",
       "0.7        0\n",
       "0.8        0\n",
       "0.9        0\n",
       "0.10       0\n",
       "0.11       0\n",
       "0.12       0\n",
       "0.13       0\n",
       "0.14       0\n",
       "0.15       0\n",
       "0.16       0\n",
       "0.17       0\n",
       "0.18       0\n",
       "0.19       0\n",
       "0.20       0\n",
       "0.21       0\n",
       "0.22       0\n",
       "0.23       0\n",
       "0.24       0\n",
       "0.25       0\n",
       "0.26       0\n",
       "0.27       0\n",
       "0.28       0\n",
       "0.29       0\n",
       "0.30       0\n",
       "0.31       0\n",
       "0.32       0\n",
       "0.33       0\n",
       "0.34       0\n",
       "0.35       0\n",
       "0.36       0\n",
       "0.37       0\n",
       "0.38       0\n",
       "0.39       0\n",
       "0.40       0\n",
       "0.41       0\n",
       "0.42       0\n",
       "0.43       0\n",
       "-1         0\n",
       "-1.1       0\n",
       "-1.2       0\n",
       "-1.3       0\n",
       "-1.4       0\n",
       "44         0\n",
       "4          0\n",
       "0.44       0\n",
       "0.5        0\n",
       "4.1        0\n",
       "-1.5       0\n",
       "40         0\n",
       "1          0\n",
       "testVal    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viz\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.10</th>\n",
       "      <th>0.11</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.13</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.15</th>\n",
       "      <th>0.16</th>\n",
       "      <th>0.17</th>\n",
       "      <th>0.18</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.22</th>\n",
       "      <th>0.23</th>\n",
       "      <th>0.24</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.26</th>\n",
       "      <th>0.27</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.29</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.33</th>\n",
       "      <th>0.34</th>\n",
       "      <th>0.35</th>\n",
       "      <th>0.36</th>\n",
       "      <th>0.37</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.39</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>-1</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>44</th>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.46</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.47</th>\n",
       "      <th>0.48</th>\n",
       "      <th>0.49</th>\n",
       "      <th>0.50</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.51</th>\n",
       "      <th>0.52</th>\n",
       "      <th>-1.5</th>\n",
       "      <th>39</th>\n",
       "      <th>1.2</th>\n",
       "      <th>testVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8868</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-20</td>\n",
       "      <td>-5</td>\n",
       "      <td>-3</td>\n",
       "      <td>12</td>\n",
       "      <td>-53</td>\n",
       "      <td>12</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.841548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8869</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-20</td>\n",
       "      <td>-5</td>\n",
       "      <td>-3</td>\n",
       "      <td>12</td>\n",
       "      <td>-53</td>\n",
       "      <td>12</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.841548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8870</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-20</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>-53</td>\n",
       "      <td>12</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.841548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8871</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-20</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>-53</td>\n",
       "      <td>12</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.841548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8872</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-20</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-53</td>\n",
       "      <td>12</td>\n",
       "      <td>-46</td>\n",
       "      <td>16</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.841548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.1  0.2  0.3  0.4  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  \\\n",
       "8868  0   16   13    9   10   10   13    8    9     9     9    11    10    11   \n",
       "8869  0   16   13    9   10   10   13    8    9     9     9    11    10    11   \n",
       "8870  0   16   13    9   10   10   13    8    9     9     9    11    10    11   \n",
       "8871  0   16   13    9   10   10   13    8    9     9     9    11    10    11   \n",
       "8872  0   15   13    9   10   10   13    8    9     9     9    11    10    11   \n",
       "\n",
       "      0.15  0.16  0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  \\\n",
       "8868     9    10    14    14    12    12     9     9    11    11     7    11   \n",
       "8869     9    10    14    14    12    12     9     9    11    11     7    11   \n",
       "8870     9    10    14    14    12    12     9     9    11    11     7    11   \n",
       "8871     9    10    14    14    12    12     9     9    11    11     7    11   \n",
       "8872     9    10    14    14    12    12     9     9    11    11     7    11   \n",
       "\n",
       "      0.27  0.28  0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  \\\n",
       "8868     7    11     1    12   -20    -5    -3    12   -53    12   -46    16   \n",
       "8869     7    14     1    12   -20    -5    -3    12   -53    12   -46    16   \n",
       "8870     7    14     1    12   -20    -5    -1    12   -53    12   -46    16   \n",
       "8871     7    14     1    12   -20    -5     0    12   -53    12   -46    16   \n",
       "8872     7    14     1    12   -20    -5     1    12   -53    12   -46    16   \n",
       "\n",
       "      0.39  0.40  0.41  0.42  0.43  -1  -1.1  -1.2  -1.3  -1.4  44   4  1  \\\n",
       "8868   -16    22    48    21    48  -1    -1    -1    -1    -1  27  70  1   \n",
       "8869   -16    22    48    21    48  -1    -1    -1    -1    -1  27  70  0   \n",
       "8870   -16    22    48    21    48  -1    -1    -1    -1    -1  27  70  0   \n",
       "8871   -16    22    48    21    48  -1    -1    -1    -1    -1  27  70  0   \n",
       "8872   -16    22    48    21    48  -1    -1    -1    -1    -1  27  70  0   \n",
       "\n",
       "      0.44  0.45  0.46       0.5  0.47  0.48  0.49  0.50  1.1  0.51  0.52  \\\n",
       "8868     0     0     0  0.817078     0     0     0     0    1     0     0   \n",
       "8869     1     0     0  0.817532     0     0     0     0    0     0     1   \n",
       "8870     1     0     0  0.818228     0     0     0     0    0     0     1   \n",
       "8871     0     1     0  0.831238     0     0     0     1    0     0     0   \n",
       "8872     1     0     0  0.831207     1     0     0     0    0     0     0   \n",
       "\n",
       "      -1.5  39  1.2   testVal  \n",
       "8868    -1  26    3  0.841548  \n",
       "8869     0  31    3  0.841548  \n",
       "8870     0  31    1  0.841548  \n",
       "8871     0  31    1  0.841548  \n",
       "8872    -1  -1   -1  0.841548  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viz\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('testVal')\n",
    "test_labels = test_features.pop('testVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.386207</td>\n",
       "      <td>1.174623</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>1.034219</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.148276</td>\n",
       "      <td>1.804094</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.127586</td>\n",
       "      <td>1.593976</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.055172</td>\n",
       "      <td>0.502992</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>1.301583</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.006897</td>\n",
       "      <td>1.112972</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.368966</td>\n",
       "      <td>1.712483</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>1.317075</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>290.0</td>\n",
       "      <td>2.068966</td>\n",
       "      <td>1.256981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>290.0</td>\n",
       "      <td>3.010345</td>\n",
       "      <td>2.295602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>290.0</td>\n",
       "      <td>3.144828</td>\n",
       "      <td>3.624177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>290.0</td>\n",
       "      <td>2.582759</td>\n",
       "      <td>2.010796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.337931</td>\n",
       "      <td>1.398074</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.031034</td>\n",
       "      <td>1.532486</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.372414</td>\n",
       "      <td>2.017038</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>1.503049</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.362069</td>\n",
       "      <td>1.906992</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.472414</td>\n",
       "      <td>1.153120</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.172414</td>\n",
       "      <td>1.466322</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.296552</td>\n",
       "      <td>1.784687</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.23</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.441379</td>\n",
       "      <td>1.306761</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.24</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>2.420175</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.162069</td>\n",
       "      <td>1.683815</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.058621</td>\n",
       "      <td>1.826377</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.27</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.679310</td>\n",
       "      <td>1.204340</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.28</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.175862</td>\n",
       "      <td>1.062137</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.29</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.362069</td>\n",
       "      <td>2.587555</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.641379</td>\n",
       "      <td>2.221866</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>1.564735</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.32</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.003448</td>\n",
       "      <td>1.383287</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.33</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.375862</td>\n",
       "      <td>1.199814</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.34</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.165517</td>\n",
       "      <td>1.702893</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.986207</td>\n",
       "      <td>1.720973</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.006897</td>\n",
       "      <td>4.258915</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.37</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.282759</td>\n",
       "      <td>1.393030</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.38</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-0.468966</td>\n",
       "      <td>2.484708</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.39</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.072414</td>\n",
       "      <td>1.080624</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.389655</td>\n",
       "      <td>0.529310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>290.0</td>\n",
       "      <td>2.310345</td>\n",
       "      <td>3.145860</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.42</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.082759</td>\n",
       "      <td>0.399020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.43</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.148276</td>\n",
       "      <td>0.514913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.2</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.3</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.4</th>\n",
       "      <td>290.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>290.0</td>\n",
       "      <td>42.300000</td>\n",
       "      <td>1.935732</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.44</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.624138</td>\n",
       "      <td>0.776005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.585966</td>\n",
       "      <td>0.170838</td>\n",
       "      <td>0.197947</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.509959</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.902794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>4.424138</td>\n",
       "      <td>1.174750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.5</th>\n",
       "      <td>290.0</td>\n",
       "      <td>8.441379</td>\n",
       "      <td>13.116922</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>290.0</td>\n",
       "      <td>13.617241</td>\n",
       "      <td>13.463588</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.589655</td>\n",
       "      <td>1.495286</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>testVal</th>\n",
       "      <td>290.0</td>\n",
       "      <td>0.592142</td>\n",
       "      <td>0.172180</td>\n",
       "      <td>0.219888</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.545454</td>\n",
       "      <td>0.742457</td>\n",
       "      <td>0.902794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count       mean        std        min   25%        50%        75%  \\\n",
       "0        290.0   0.000000   0.000000   0.000000   0.0   0.000000   0.000000   \n",
       "0.1      290.0  -0.386207   1.174623  -2.000000  -1.0  -1.000000   1.000000   \n",
       "0.2      290.0   0.744828   1.034219  -1.000000   0.0   1.000000   1.000000   \n",
       "0.3      290.0   0.148276   1.804094  -5.000000  -1.0   0.000000   1.000000   \n",
       "0.4      290.0  -0.127586   1.593976  -3.000000  -1.0  -1.000000   1.000000   \n",
       "0.6      290.0  -1.055172   0.502992  -4.000000  -1.0  -1.000000  -1.000000   \n",
       "0.7      290.0  -0.600000   1.301583  -3.000000  -2.0  -1.000000   1.000000   \n",
       "0.8      290.0  -1.006897   1.112972  -4.000000  -2.0  -1.000000  -1.000000   \n",
       "0.9      290.0  -0.368966   1.712483  -3.000000  -2.0   0.000000   1.000000   \n",
       "0.10     290.0   0.048276   1.317075  -3.000000  -1.0   0.000000   1.000000   \n",
       "0.11     290.0   2.068966   1.256981   0.000000   1.0   2.000000   2.000000   \n",
       "0.12     290.0   3.010345   2.295602   0.000000   1.0   2.000000   5.000000   \n",
       "0.13     290.0   3.144828   3.624177   0.000000   1.0   2.000000   2.000000   \n",
       "0.14     290.0   2.582759   2.010796   0.000000   1.0   2.000000   3.000000   \n",
       "0.15     290.0   0.337931   1.398074  -3.000000  -1.0   1.000000   1.000000   \n",
       "0.16     290.0  -0.031034   1.532486  -3.000000  -1.0   0.000000   1.000000   \n",
       "0.17     290.0   0.372414   2.017038  -3.000000  -1.0   0.000000   2.000000   \n",
       "0.18     290.0   0.896552   1.503049  -3.000000   0.0   1.000000   2.000000   \n",
       "0.19     290.0  -0.362069   1.906992  -6.000000  -1.0  -1.000000   1.000000   \n",
       "0.20     290.0  -0.472414   1.153120  -2.000000  -1.0  -1.000000   0.000000   \n",
       "0.21     290.0   1.172414   1.466322  -2.000000   0.0   1.000000   3.000000   \n",
       "0.22     290.0  -0.296552   1.784687  -4.000000  -1.0   0.000000   1.000000   \n",
       "0.23     290.0  -0.441379   1.306761  -2.000000  -2.0  -1.000000   1.000000   \n",
       "0.24     290.0   0.613793   2.420175  -3.000000  -1.0  -0.500000   2.000000   \n",
       "0.25     290.0  -0.162069   1.683815  -4.000000  -1.0  -1.000000   1.000000   \n",
       "0.26     290.0   1.058621   1.826377  -3.000000   0.0   1.000000   2.000000   \n",
       "0.27     290.0  -0.679310   1.204340  -3.000000  -1.0  -1.000000   0.000000   \n",
       "0.28     290.0  -0.175862   1.062137  -1.000000  -1.0  -1.000000   1.000000   \n",
       "0.29     290.0  -1.362069   2.587555  -7.000000  -2.0  -1.000000   0.000000   \n",
       "0.30     290.0  -0.641379   2.221866  -6.000000  -2.0  -1.000000   1.000000   \n",
       "0.31     290.0   0.206897   1.564735  -3.000000  -1.0   0.000000   1.000000   \n",
       "0.32     290.0  -1.003448   1.383287  -3.000000  -2.0  -1.000000   0.000000   \n",
       "0.33     290.0  -0.375862   1.199814  -3.000000  -1.0  -1.000000   0.000000   \n",
       "0.34     290.0   0.165517   1.702893  -3.000000  -1.0   0.000000   2.000000   \n",
       "0.35     290.0  -0.986207   1.720973  -4.000000  -1.0  -1.000000   0.000000   \n",
       "0.36     290.0   1.006897   4.258915  -4.000000  -1.0  -1.000000   2.000000   \n",
       "0.37     290.0  -0.282759   1.393030  -4.000000  -1.0  -1.000000   1.000000   \n",
       "0.38     290.0  -0.468966   2.484708  -3.000000  -2.0  -2.000000   1.000000   \n",
       "0.39     290.0  -1.072414   1.080624  -2.000000  -2.0  -1.000000  -1.000000   \n",
       "0.40     290.0   1.389655   0.529310   0.000000   1.0   1.000000   2.000000   \n",
       "0.41     290.0   2.310345   3.145860  -2.000000   0.0   2.000000   2.000000   \n",
       "0.42     290.0   1.082759   0.399020   0.000000   1.0   1.000000   1.000000   \n",
       "0.43     290.0   1.148276   0.514913   0.000000   1.0   1.000000   1.000000   \n",
       "-1       290.0  -1.000000   0.000000  -1.000000  -1.0  -1.000000  -1.000000   \n",
       "-1.1     290.0  -1.000000   0.000000  -1.000000  -1.0  -1.000000  -1.000000   \n",
       "-1.2     290.0  -1.000000   0.000000  -1.000000  -1.0  -1.000000  -1.000000   \n",
       "-1.3     290.0  -1.000000   0.000000  -1.000000  -1.0  -1.000000  -1.000000   \n",
       "-1.4     290.0  -1.000000   0.000000  -1.000000  -1.0  -1.000000  -1.000000   \n",
       "44       290.0  42.300000   1.935732  38.000000  41.0  43.000000  44.000000   \n",
       "4        290.0   4.000000   0.000000   4.000000   4.0   4.000000   4.000000   \n",
       "0.44     290.0   0.624138   0.776005   0.000000   0.0   0.000000   1.000000   \n",
       "0.5      290.0   0.585966   0.170838   0.197947   0.5   0.509959   0.726623   \n",
       "4.1      290.0   4.424138   1.174750   0.000000   4.0   4.000000   6.000000   \n",
       "-1.5     290.0   8.441379  13.116922  -1.000000  -1.0  -1.000000  17.000000   \n",
       "40       290.0  13.617241  13.463588  -1.000000   0.0  11.000000  24.000000   \n",
       "1        290.0   1.589655   1.495286  -1.000000   1.0   1.000000   2.000000   \n",
       "testVal  290.0   0.592142   0.172180   0.219888   0.5   0.545454   0.742457   \n",
       "\n",
       "               max  \n",
       "0         0.000000  \n",
       "0.1       1.000000  \n",
       "0.2       3.000000  \n",
       "0.3       9.000000  \n",
       "0.4       8.000000  \n",
       "0.6       0.000000  \n",
       "0.7       2.000000  \n",
       "0.8       3.000000  \n",
       "0.9       3.000000  \n",
       "0.10      3.000000  \n",
       "0.11      6.000000  \n",
       "0.12      8.000000  \n",
       "0.13     22.000000  \n",
       "0.14      7.000000  \n",
       "0.15      3.000000  \n",
       "0.16      5.000000  \n",
       "0.17      5.000000  \n",
       "0.18      4.000000  \n",
       "0.19      6.000000  \n",
       "0.20      4.000000  \n",
       "0.21      3.000000  \n",
       "0.22      3.000000  \n",
       "0.23      4.000000  \n",
       "0.24      9.000000  \n",
       "0.25      4.000000  \n",
       "0.26      7.000000  \n",
       "0.27      2.000000  \n",
       "0.28      3.000000  \n",
       "0.29      3.000000  \n",
       "0.30      4.000000  \n",
       "0.31      5.000000  \n",
       "0.32      1.000000  \n",
       "0.33      3.000000  \n",
       "0.34      3.000000  \n",
       "0.35      2.000000  \n",
       "0.36     16.000000  \n",
       "0.37      6.000000  \n",
       "0.38      7.000000  \n",
       "0.39      2.000000  \n",
       "0.40      2.000000  \n",
       "0.41     11.000000  \n",
       "0.42      3.000000  \n",
       "0.43      3.000000  \n",
       "-1       -1.000000  \n",
       "-1.1     -1.000000  \n",
       "-1.2     -1.000000  \n",
       "-1.3     -1.000000  \n",
       "-1.4     -1.000000  \n",
       "44       44.000000  \n",
       "4         4.000000  \n",
       "0.44      3.000000  \n",
       "0.5       0.902794  \n",
       "4.1       6.000000  \n",
       "-1.5     41.000000  \n",
       "40       41.000000  \n",
       "1        13.000000  \n",
       "testVal   0.902794  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00 -3.8620690e-01  7.4482757e-01  1.4827587e-01\n",
      "  -1.2758620e-01 -1.0551724e+00 -6.0000002e-01 -1.0068966e+00\n",
      "  -3.6896554e-01  4.8275862e-02  2.0689654e+00  3.0103450e+00\n",
      "   3.1448276e+00  2.5827584e+00  3.3793104e-01 -3.1034481e-02\n",
      "   3.7241381e-01  8.9655173e-01 -3.6206895e-01 -4.7241381e-01\n",
      "   1.1724137e+00 -2.9655173e-01 -4.4137931e-01  6.1379313e-01\n",
      "  -1.6206896e-01  1.0586207e+00 -6.7931038e-01 -1.7586207e-01\n",
      "  -1.3620689e+00 -6.4137936e-01  2.0689654e-01 -1.0034482e+00\n",
      "  -3.7586209e-01  1.6551724e-01 -9.8620695e-01  1.0068966e+00\n",
      "  -2.8275862e-01 -4.6896556e-01 -1.0724138e+00  1.3896552e+00\n",
      "   2.3103449e+00  1.0827585e+00  1.1482759e+00 -1.0000000e+00\n",
      "  -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00\n",
      "   4.2299999e+01  4.0000000e+00  6.2413788e-01  5.8596635e-01\n",
      "   8.4413795e+00  1.3617242e+01  1.5896552e+00  1.3793102e-02\n",
      "   1.3103448e-01  5.4137927e-01  1.7241379e-02  2.9655176e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = np.array(train_features[:1])\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.13239077],\n",
       "       [ 1.5114526 ],\n",
       "       [ 0.4887989 ],\n",
       "       [ 1.227671  ],\n",
       "       [-0.5342397 ],\n",
       "       [-0.59074736],\n",
       "       [ 0.07944753],\n",
       "       [ 1.0066601 ],\n",
       "       [ 0.44309202],\n",
       "       [-1.3325415 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/kernel:0' shape=(65, 1) dtype=float32, numpy=\n",
       "array([[-0.16479614],\n",
       "       [-0.18584204],\n",
       "       [ 0.17210305],\n",
       "       [ 0.07251981],\n",
       "       [-0.282098  ],\n",
       "       [ 0.17026451],\n",
       "       [ 0.19518223],\n",
       "       [ 0.02784142],\n",
       "       [-0.2202789 ],\n",
       "       [-0.28409418],\n",
       "       [-0.1358776 ],\n",
       "       [-0.20928985],\n",
       "       [-0.04382697],\n",
       "       [ 0.08444387],\n",
       "       [ 0.28072935],\n",
       "       [ 0.2541737 ],\n",
       "       [ 0.18931714],\n",
       "       [-0.21859735],\n",
       "       [-0.03636074],\n",
       "       [ 0.2258851 ],\n",
       "       [-0.14081752],\n",
       "       [ 0.28928393],\n",
       "       [ 0.20873535],\n",
       "       [ 0.28305095],\n",
       "       [ 0.03377193],\n",
       "       [-0.05631657],\n",
       "       [-0.1704995 ],\n",
       "       [ 0.13312271],\n",
       "       [-0.02234221],\n",
       "       [-0.12153865],\n",
       "       [ 0.05791065],\n",
       "       [-0.2674414 ],\n",
       "       [ 0.2807294 ],\n",
       "       [-0.12202683],\n",
       "       [ 0.21918285],\n",
       "       [ 0.05712384],\n",
       "       [-0.06301713],\n",
       "       [ 0.21663547],\n",
       "       [-0.24643531],\n",
       "       [ 0.03888386],\n",
       "       [ 0.00321001],\n",
       "       [-0.21219613],\n",
       "       [-0.2518988 ],\n",
       "       [-0.16679838],\n",
       "       [-0.13316326],\n",
       "       [ 0.1379939 ],\n",
       "       [ 0.28396899],\n",
       "       [ 0.13678968],\n",
       "       [-0.15005954],\n",
       "       [ 0.16344592],\n",
       "       [-0.08935864],\n",
       "       [ 0.06941858],\n",
       "       [-0.13640308],\n",
       "       [-0.14185736],\n",
       "       [ 0.21172231],\n",
       "       [ 0.27959633],\n",
       "       [ 0.05900365],\n",
       "       [-0.05176252],\n",
       "       [ 0.04342571],\n",
       "       [-0.16884713],\n",
       "       [-0.06607802],\n",
       "       [-0.00734028],\n",
       "       [ 0.20017874],\n",
       "       [-0.26278058],\n",
       "       [-0.1267795 ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.layers[1].kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\data_adapter.py:1696: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 28.1 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = linear_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results['linear_model'] = linear_model.evaluate(\n",
    "    test_features, test_labels, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "  model = keras.Sequential([\n",
    "      norm,\n",
    "      layers.Dense(512, activation='relu'),\n",
    "      layers.Dense(512, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 65)               131       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               33792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 297,092\n",
      "Trainable params: 296,961\n",
      "Non-trainable params: 131\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn_model = build_and_compile_model(normalizer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\data_adapter.py:1696: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 7ms/step - loss: 0.1228 - val_loss: 0.0727\n",
      "Epoch 2/100\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 0.0678 - val_loss: 0.0613\n",
      "Epoch 3/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0591 - val_loss: 0.0549\n",
      "Epoch 4/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0502 - val_loss: 0.0486\n",
      "Epoch 5/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0456 - val_loss: 0.0477\n",
      "Epoch 6/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0433 - val_loss: 0.0420\n",
      "Epoch 7/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0395 - val_loss: 0.0396\n",
      "Epoch 8/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0381 - val_loss: 0.0372\n",
      "Epoch 9/100\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 0.0358 - val_loss: 0.0402\n",
      "Epoch 10/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0359 - val_loss: 0.0416\n",
      "Epoch 11/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0348 - val_loss: 0.0342\n",
      "Epoch 12/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0320 - val_loss: 0.0393\n",
      "Epoch 13/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0319 - val_loss: 0.0301\n",
      "Epoch 14/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0308 - val_loss: 0.0357\n",
      "Epoch 15/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0298 - val_loss: 0.0291\n",
      "Epoch 16/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0287 - val_loss: 0.0302\n",
      "Epoch 17/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.0294\n",
      "Epoch 18/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0262 - val_loss: 0.0286\n",
      "Epoch 19/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0261 - val_loss: 0.0303\n",
      "Epoch 20/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0261 - val_loss: 0.0297\n",
      "Epoch 21/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0248 - val_loss: 0.0249\n",
      "Epoch 22/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0300\n",
      "Epoch 23/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0243 - val_loss: 0.0279\n",
      "Epoch 24/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.0292\n",
      "Epoch 25/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0238 - val_loss: 0.0261\n",
      "Epoch 26/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 27/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0219 - val_loss: 0.0253\n",
      "Epoch 28/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0217 - val_loss: 0.0243\n",
      "Epoch 29/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0212 - val_loss: 0.0230\n",
      "Epoch 30/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.0267\n",
      "Epoch 31/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0211 - val_loss: 0.0220\n",
      "Epoch 32/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 33/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0211 - val_loss: 0.0208\n",
      "Epoch 34/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.0252\n",
      "Epoch 35/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0191 - val_loss: 0.0240\n",
      "Epoch 36/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0194 - val_loss: 0.0211\n",
      "Epoch 37/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0198 - val_loss: 0.0215\n",
      "Epoch 38/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0181 - val_loss: 0.0213\n",
      "Epoch 39/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0185 - val_loss: 0.0212\n",
      "Epoch 40/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0179 - val_loss: 0.0208\n",
      "Epoch 41/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.0213\n",
      "Epoch 42/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0180 - val_loss: 0.0205\n",
      "Epoch 43/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0178 - val_loss: 0.0186\n",
      "Epoch 44/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0172 - val_loss: 0.0208\n",
      "Epoch 45/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.0187\n",
      "Epoch 46/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0167 - val_loss: 0.0177\n",
      "Epoch 47/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0170 - val_loss: 0.0215\n",
      "Epoch 48/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0166 - val_loss: 0.0210\n",
      "Epoch 49/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0160 - val_loss: 0.0209\n",
      "Epoch 50/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0160 - val_loss: 0.0223\n",
      "Epoch 51/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 52/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0164 - val_loss: 0.0223\n",
      "Epoch 53/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0157 - val_loss: 0.0204\n",
      "Epoch 54/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0158 - val_loss: 0.0187\n",
      "Epoch 55/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0158 - val_loss: 0.0192\n",
      "Epoch 56/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0153 - val_loss: 0.0197\n",
      "Epoch 57/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0154 - val_loss: 0.0199\n",
      "Epoch 58/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0151 - val_loss: 0.0185\n",
      "Epoch 59/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0149 - val_loss: 0.0185\n",
      "Epoch 60/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0149 - val_loss: 0.0173\n",
      "Epoch 61/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0143 - val_loss: 0.0183\n",
      "Epoch 62/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0148 - val_loss: 0.0205\n",
      "Epoch 63/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0143 - val_loss: 0.0176\n",
      "Epoch 64/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0140 - val_loss: 0.0166\n",
      "Epoch 65/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0138 - val_loss: 0.0167\n",
      "Epoch 66/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0139 - val_loss: 0.0187\n",
      "Epoch 67/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0157 - val_loss: 0.0198\n",
      "Epoch 68/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 69/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0136 - val_loss: 0.0164\n",
      "Epoch 70/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0139 - val_loss: 0.0179\n",
      "Epoch 71/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0135 - val_loss: 0.0162\n",
      "Epoch 72/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0132 - val_loss: 0.0191\n",
      "Epoch 73/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0134 - val_loss: 0.0161\n",
      "Epoch 74/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0131 - val_loss: 0.0147\n",
      "Epoch 75/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 76/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0141 - val_loss: 0.0174\n",
      "Epoch 77/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.0189\n",
      "Epoch 78/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0138 - val_loss: 0.0197\n",
      "Epoch 79/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.0154\n",
      "Epoch 80/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0128 - val_loss: 0.0149\n",
      "Epoch 81/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.0148\n",
      "Epoch 82/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0125 - val_loss: 0.0150\n",
      "Epoch 83/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0130 - val_loss: 0.0182\n",
      "Epoch 84/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 85/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 86/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0126 - val_loss: 0.0144\n",
      "Epoch 87/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0122 - val_loss: 0.0153\n",
      "Epoch 88/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0128 - val_loss: 0.0157\n",
      "Epoch 89/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0121 - val_loss: 0.0173\n",
      "Epoch 90/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0118 - val_loss: 0.0149\n",
      "Epoch 91/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0119 - val_loss: 0.0143\n",
      "Epoch 92/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0125 - val_loss: 0.0156\n",
      "Epoch 93/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0123 - val_loss: 0.0187\n",
      "Epoch 94/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0118 - val_loss: 0.0156\n",
      "Epoch 95/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 96/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0110 - val_loss: 0.0152\n",
      "Epoch 97/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0119 - val_loss: 0.0140\n",
      "Epoch 98/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0114 - val_loss: 0.0145\n",
      "Epoch 99/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0123 - val_loss: 0.0204\n",
      "Epoch 100/100\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0129 - val_loss: 0.0178\n",
      "CPU times: total: 3min 4s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean absolute error [MPG]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear_model</th>\n",
       "      <td>0.412108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dnn_model</th>\n",
       "      <td>0.017952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mean absolute error [MPG]\n",
       "linear_model                   0.412108\n",
       "dnn_model                      0.017952"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = dnn_model.predict(test_features).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_model3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dnn_model3\\assets\n"
     ]
    }
   ],
   "source": [
    "dnn_model.save('dnn_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model('dnn_model3')\n",
    "\n",
    "test_results['reloaded'] = reloaded.evaluate(\n",
    "    test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.35849035]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [0, -2, 1, -2, 1, -2, -2, 1, 1, 3, 2, 6, 1, 3, -2, -3, -3, 2, -2, -1, 2, 2, -2, -3, 1, -3, -2, 3, -2, -3, -3, -1, -1, 3, 2, -2, 1, -3, -2, 1, 8, 1, 1, -1, -1, -1, -1, -1, 44, 4, 0, 0.3341376050825775, 4, -1, 16, 1]\n",
    "    #data[i] = str(data[i])\n",
    "dnn_model.predict(np.array( [data,] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3aee800614a5485d4e1ef423aa2f6866bf568d4db94c3995a189d53298965bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
